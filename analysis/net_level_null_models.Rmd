---
title: "Null model network comparisons"
author: "Elena Quintero"
date: "`r Sys.Date()`"
output: github_document
---


```{r include = F}
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```


```{r, message=F}
library(here)
library(tidyverse)
library(magrittr)
library(bipartite)
library(igraph)
library(tnet)
```


## NULL MODELS

Read matrices:

First I create a function to calculate for each matrix, the scale factor necessary so that every non-zero value has at least number different that zero before the decimals. Then I do a ceiling (function to round interaction to integers). By using a scale factor decimals are not lost.

```{r}
#Read list of names

net_names <- list.files(path = here("networks/nets_std"), pattern = "_int")

#Create empty list

nets <- list()

#Add all read files to the list

for (i in 1:length(net_names)){
  
  net_file <- paste0("networks/nets_std/", net_names[[i]])
  net <- read.csv(here(net_file))
  
  if (str_detect(net_names[i], "sp_")) {
    
    net %<>% column_to_rownames("sp")
    
  }
  
  else {
    
    net %<>% column_to_rownames("ind")
    
  }
  
  nets[[i]] <- net #add to list
  names(nets)[i] <- net_names[[i]] #name the list
  
}

```

First I create a function to calculate for each matrix, the scale factor necessary so that every non-zero value has at least number different that zero before the decimals. Then I do a ceiling (function to round interaction to integers). By using a scale factor small decimals are not lost.

```{r}
calc_order_magnitude <- function(matrix) {
  # Excluir ceros para evitar división por cero
  non_zero_values <- matrix[matrix != 0]
  
  # Obtener el valor mínimo absoluto de los valores no cero
  min_value <- min(abs(non_zero_values))
  
  # Calcular el orden de magnitud necesario para convertirlo en un número mayor o igual a 1
  order_magnitude <- 10^ceiling(-log10(min_value))
  
  return(order_magnitude)
}

# Calcular el factor de escala para todas las redes
scale_factors <- lapply(nets, calc_order_magnitude)

```

```{r}
nets_multiplied <- list()

for (i in 1:length(nets)){
  
  nets_multiplied[[i]] <- ceiling(nets[[i]] * scale_factors[[i]])
  
  names(nets_multiplied)[i] <- net_names[[i]] #name the list

}
```


Generate 1000 random nets following *Patefield*'s algorithm (keeps constant marginal rows and columns)

```{r}
null.list <- list()
#null.list <- vector("list", 5)

for (i in 1:length(nets_multiplied)) {
  
  print(i)
  
  nulls <- nullmodel(nets_multiplied[[i]], N = 1000, method = "r2dtable") 
  
  scale_back <- function(list){
    list / scale_factors[[i]]
    }
  
  nulls.fix <- lapply(nulls, scale_back)
  
  null.list <- append(null.list, list(nulls.fix))
  #null.list[[i]] <- nulls.fix
  
  names(null.list)[[i]] <- names(nets_multiplied)[i]
  
}
```

Generate 1000 random nets following *Vazquez* algorithm (keeps constant marginal rows and columns)

```{r}
#vaznull.list <- list()
vaznull.list <- vector("list", 5)

for (i in 1:length(nets_multiplied)) {
  
  print(i)
  
  nulls <- nullmodel(nets_multiplied[[i]], N = 1000, method = "vaznull")
  
  scale_back <- function(list){
    list / scale_factors[[i]]
    }
  
  nulls.fix <- lapply(nulls, scale_back)
  
  #vaznull.list <- append(vaznull.list, list(nulls.fix))
  vaznull.list[[i]] <- nulls.fix
  
  names(vaznull.list)[[i]] <- names(nets_multiplied)[i]
  
}
```

Check Vazquez null models sum up to 1:
```{r}
sum(vaznull.list[[1]][[1]])
sum(vaznull.list[[81]][[981]])
```

Compare null models for same net using different scale factors (e.g. 1000, 10000 and 100000):
```{r, eval=FALSE, include=FALSE}
nulls1 <- nullmodel(nets[[7]]*1000, N = 10, method = "vaznull")
nulls2 <- nullmodel(nets[[7]]*10000, N = 10, method = "vaznull")
nulls3 <- nullmodel(nets[[7]]*100000, N = 10, method = "vaznull")

sum(nets[[7]])
sum(nulls1[[1]]/1000)
sum(nulls1[[3]]/1000)
sum(nulls2[[1]]/10000)
sum(nulls2[[3]]/10000)
sum(nulls3[[5]]/100000)
sum(nulls3[[9]]/100000)

networklevel(nets[[7]], index = "connectance")
networklevel(nulls1[[1]]/1000, index = "connectance")
networklevel(nulls2[[1]]/10000, index = "connectance")
networklevel(nulls3[[1]]/100000, index = "connectance")

networklevel(nets[[7]], index = "weigthed.NODF")
networklevel(nulls1[[1]]/1000, index = "weigthed.NODF")
networklevel(nulls2[[1]]/10000, index = "weigthed.NODF")
networklevel(nulls3[[1]]/100000, index = "weigthed.NODF")
```

## Calculate network level metrics:

```{r}
indices <- as.list(c("number of species",
                     "connectance", 
                     "weighted NODF",
                     "interaction evenness", 
                     "Alatalo interaction evenness"))
```

```{r}
# Progress bar initialization
pb <- txtProgressBar(min = 0, max = length(nets), style = 3)

# Initialize web.metrics list
web.metrics <- lapply(1:length(vaznull.list), function(x) vector("list", 1000))

# Function to compute bipartite metrics
compute_bipartite_metrics <- function(net) {
  m.bipart <- networklevel(net, indices)
  return(as.data.frame(t(m.bipart)))
}

# Function to compute modularity
compute_modularity <- function(net) {
  mod <- computeModules(net)
  return(mod@likelihood)
}

# Function to compute igraph metrics
compute_igraph_metrics <- function(net) {
  inet <- graph_from_biadjacency_matrix(net, weighted = TRUE, add.names = NULL)
  assortativity <- assortativity_degree(inet)
  eigen.centrality <- eigen_centrality(inet)$value
  centr_binary <- centr_eigen(inet, scale = TRUE, normalized = TRUE)$centralization
  centr_weighted_obs <- sum(max(eigen_centrality(inet)$vector) - eigen_centrality(inet)$vector)
  centralization_w <- centr_weighted_obs / centr_eigen(inet)$theoretical_max
  return(cbind(assortativity, eigen.centrality, centr_binary, centralization_w))
}

# Umbrella function to process each network and iteration
process_network <- function(i, null_list) {
  
  net_nulls <- null_list[[i]]
  
  metrics_list <- lapply(1:1000, function(j) {
    
    m.bipart <- compute_bipartite_metrics(net_nulls[[j]])
    
    M <- compute_modularity(net_nulls[[j]])
    
    igraph.metrics <- compute_igraph_metrics(net_nulls[[j]])
    
    # Merge all metrics
    metrics.nulls <- cbind(m.bipart, M, igraph.metrics) %>%
      select_all(~gsub("\\s+", ".", .)) %>%
      mutate(net_size = number.of.species.HL * number.of.species.LL,
             iter = j)
    
    # Add type and net_id
    if (str_detect(names(null_list)[i], "_int")) {
      metrics.nulls <- metrics.nulls %>%
        mutate(type = "ind", net_id = str_sub(names(null_list)[i], 1, 5))
    } else {
      metrics.nulls <- metrics.nulls %>%
        mutate(type = "sp", net_id = str_sub(names(null_list)[i], 4, 8))
    }
    
    return(metrics.nulls)
  })
  
  return(metrics_list)
}
```

Run umbrella function for each *Vazquez* null model network:

```{r}
# Main lapply for each network
web.metrics <- lapply(1:length(vaznull.list), function(i) {
  metrics <- process_network(i, vaznull.list)
  
  # Update progress bar
  setTxtProgressBar(pb, i)
  
  return(metrics)
})

# Close progress bar
close(pb)
```

Makes one data.table from a list of many

```{r}
web.metrics.df <- data.table::rbindlist(
  lapply(1:length(web.metrics), function(i) {
  data.table::rbindlist(web.metrics[[i]])
  })
  )

web.metrics.df <- web.metrics.df |> 
  relocate(type, .before = everything()) |> 
  relocate(net_id, .after = type) |> 
  relocate(iter, .after = net_id) |> 
  mutate(net_code = paste0(type, "_", net_id)) |> 
  group_by(iter) |> 
  mutate(net_n = order(net_code))

glimpse(web.metrics.df)
```


Run umbrella function for each *Patefield* null model network:

```{r}
web.metrics.pat <- lapply(1:length(null.list), function(i) {
  metrics <- process_network(i, null.list)
  
  # Update progress bar
  setTxtProgressBar(pb, i)
  
  return(metrics)
})

web.metrics.pat.df <- data.table::rbindlist(
  lapply(1:length(web.metrics.pat), function(i) {
  data.table::rbindlist(web.metrics.pat[[i]])
  })
  )

web.metrics.pat.df <- web.metrics.pat.df |> 
  relocate(type, .before = everything()) |> 
  relocate(net_id, .after = type) |> 
  relocate(iter, .after = net_id) |> 
  mutate(net_code = paste0(type, "_", net_id)) |> 
  group_by(iter) |> 
  mutate(net_n = order(net_code))

glimpse(web.metrics.pat.df)
```


```{r}
saveRDS(web.metrics.df, here("data/net_level_nulls_vazquez.rds"))

saveRDS(web.metrics.pat.df, here("data/net_level_nulls.rds"))
```


```{r eval = FALSE}
pb = txtProgressBar(min = 0, max = length(nets), style = 3) 

web.metrics <- vector("list", 105)

lapply(web.metrics, FUN = function(x){vector("list", 1000)})

parallel(i in 1:length(null.list)) {
  
  net_nulls <- null.list[[i]]
  
  for (j in 1:1000) {
    
    m.bipart <- networklevel(net_nulls[[j]], indices)
    m.bipart <- as.data.frame(t(m.bipart))  
    
    # Modularity (bipartite)
    mod <- computeModules(net_nulls[[j]])
    M = mod@likelihood
    
    # Igraph metrics
    inet <- graph_from_biadjacency_matrix(net_nulls[[j]], weighted = T,  add.names=NULL)
    assortativity = assortativity_degree(inet)
    eigen.centrality = eigen_centrality(inet)$value
    centr_binary = centr_eigen(inet, scale = TRUE, normalized = TRUE)$centralization
    centr.weighted.obs = sum(max(eigen_centrality(inet)$vector) - eigen_centrality(inet)$vector)
    centralization.w = centr.weighted.obs/ centr_eigen(inet)$theoretical_max
    igraph.metrics <- cbind(assortativity, eigen.centrality, centr_binary, centralization.w)
    
    #merge all metrics
    metrics.nulls <- cbind(m.bipart, M, igraph.metrics) |> 
      select_all(~gsub("\\s+", ".", .)) |> 
      mutate(net_size = number.of.species.HL * number.of.species.LL) |> 
      mutate(iter = j) 
    
    if (str_detect(names(null.list)[i], "_int")) {
      
      metrics.nulls <- metrics.nulls |> 
        mutate(type = "ind",
               net_id = str_sub(names(null.list)[i], 1, 5))
      
    }
    
    else {
      
      metrics.nulls <- metrics.nulls |> 
        mutate(type = "sp",
               net_id = str_sub(names(null.list)[i], 4, 8))
      
    }
    
    web.metrics[[i]][[j]] <- metrics.nulls
    
  }

   setTxtProgressBar(pb,i)
  
}


web.metrics <- web.metrics |> 
  relocate(type, .before = everything()) |> 
  relocate(net_id, .after = type) |> 
  relocate(iter, .after = net_id) |> 
  mutate(net_code = paste0(type, "_", net_id)) |> 
  group_by(iter) |> 
  mutate(net_n = order(net_code))

glimpse(web.metrics)
```



