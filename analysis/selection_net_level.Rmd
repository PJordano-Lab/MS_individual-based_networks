---
title: "Exploration net-level metrics "
author: "Elena Quintero"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

Libraries:

```{r}
library(here)
library(tidyverse)
library(magrittr)
library(rcartocolor)
library(patchwork)
library(psych)
library(ggfortify)
library(ggrepel)
library(knitr)
library(cluster)
library(GGally)
library(reshape2)
library(ggcorrplot)
theme_set(theme_minimal())
```


Load net-level metrics:
```{r}
net.level <- read.csv(here("data/net.level.selection.csv"))

glimpse(net.level)
```

```{r}
summary(net.level$net_size)
summary(net.level$unique.ints)
```


Cluster analysis for metrics:

```{r cluster_analysis, fig.width = 7, fig.height = 5}
df <- net.level %>% 
  dplyr::select(-c(net_id, net_n, type, net_code, unique.ints, code_ID,
                    ref, bioregion, country, plant_sp, family:cols_bioregions_d2)) %>%
  dplyr::select(-starts_with("niche.overlap.horn")) %>%
  dplyr::select(-starts_with("niche.overlap.jaccard")) %>%
  dplyr::select(-centr_binary) %>%
  scale() %>% t()

# Dissimilarity matrix
d <- dist(df, method = "euclidean")

# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete")

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)
#rect.hclust(hc1, k = 10)
# saved as PNG in "analysis/figs/Fig_S1_cluster_net_metrics.pdf", width = 600, height = 500
```

Compare the three niche overlap metrics:

```{r niche_overlap, fig.width = 7, fig.height = 5}
net.level %>%
  select(type, starts_with("niche")) %>% melt() %>%
  ggplot(aes(x = value, fill = type)) + 
  geom_density(alpha = 0.5) + 
  facet_wrap(~variable, ncol = 2) +
  scale_fill_carto_d()
```

Rename metrics:

```{r}
net.level <- net.level |> 
  rename("Modularity" = "M") |> 
  rename("Assortativity" = "assortativity") |> 
  rename("Centralization" = "centralization.w") |> 
  rename("Connectance" = "connectance") |> 
  rename("Interaction_evenness" = "interaction.evenness") |> 
  rename("Weighted_NODF" = "weighted.NODF") |> 
  rename("Network_size" = "net_size")
```


Correlation between metrics:

```{r}
glimpse(net.level)
metrics.of.interest.size <- c("Connectance", 
                         "Weighted_NODF", 
                         "Modularity", 
                         "Interaction_evenness",
                         "Centralization",
                         "Assortativity",
                         "Network_size")

#Remove net_size
metrics.of.interest <- metrics.of.interest.size[metrics.of.interest.size != "net_size"]
```

Correlation between selected metrics:

```{r cor_selec_metrics, fig.width = 8, fig.height = 6}
corr_matrix <- cor(net.level[, metrics.of.interest.size])
p.mat <- cor_pmat(net.level[, metrics.of.interest.size], conf.level = 0.95)

ggcorrplot(corr_matrix, type = "lower",  lab = T) 

#ggsave(here("figs/Fig_S2_cor_net_metrics.png"), width = 7, height=7)
```


VIF:

```{r}
source(here("functions/vif_function.R"))

vif_func(in_frame = net.level[, metrics.of.interest], thresh = 4, trace = T)
```


Full comparison metrics:

```{r pairs_selec_metrics, fig.width = 10, fig.height = 7}
ggpairs(net.level, columns = metrics.of.interest, 
        aes(color = type, fill = type),
        diag = list(continuous = wrap("densityDiag", alpha = 0.5)),
        lower = list(continuous = wrap("points", alpha = 0.5))) +
  scale_color_carto_d() +
  scale_fill_carto_d()
```


Summary numbers:

```{r}
summary(net.level[, c("type", metrics.of.interest)])

net.level[, c("type", metrics.of.interest)] %>%
  #group_by(type) %>% 
  summarise_at(vars(Connectance:Assortativity), sd)


ind <- net.level[, c("type", metrics.of.interest)] %>% filter(type == "ind")
sp <- net.level[, c("type", metrics.of.interest)] %>% filter(type == "sp")
summary(ind)
summary(sp)

net.level[, c("type", metrics.of.interest)] %>%
  group_by(type) %>% 
  summarise_at(vars(Connectance:Assortativity), sd)


#coefficient of variation
net.level[, c("type", metrics.of.interest)] %>%
  #group_by(type) %>% 
  summarise(cv_con = sd(Connectance) / mean(Connectance) * 100,
            cv_nodf = sd(`Weighted_NODF`) / mean(`Weighted_NODF`) * 100,
            cv_mod = sd(Modularity) / mean(Modularity) * 100,
            cv_cen = sd(Centralization) / mean(Centralization) * 100,
            cv_ass = sd(Assortativity) / mean(Assortativity) * 100)
```


